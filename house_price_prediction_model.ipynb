{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering with PySpark\n",
    "\n",
    "#### This program summarizes this course on Feature Engineering, which focuses on building a model to predict how much a house sells for. \n",
    "    \n",
    "#### The dataset we have is a sample of homes that were sold in St Paul, MN area over the course of 2017. \n",
    "    \n",
    "#### Using this sample, we are to provide a quick proof of concept of whether it is worth investing in more data for the 5.5 milion homes that were sold in the US In 2017.\n",
    "    \n",
    "##### Note: Dowload data at https://assets.datacamp.com/production/repositories/1704/datasets/d26c25f46746882d0a0f474cc6709c629f69872c/2017_StPaul_MN_Real_Estate.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 0: Add PySpark to path and create a SparkSession "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add PySpark to sys.path at runtime\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Import SparkSession from pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "# Create or get a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Exploratory data analysis (EDA)\n",
    "#### (1) columns names and dtypes, statistics (describe()), number of rows and columns (5000, 74) \n",
    "#### (2) correlations between Xi and Y, distribution and distplot(), and lmplot() (linear model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1.1) Read in CSV as Spark DataFrame (first row as column names)\n",
    "df = spark.read.csv('../../data/2017_StPaul_MN_Real_Estate.csv', \n",
    "                    header=True, inferSchema=True)\n",
    "\n",
    "# (1.2) Correct date columns dtype\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# convert LISTDATE to date\n",
    "df = df.withColumn('LISTDATE', to_date('LISTDATE', 'MM/dd/yyyy HH:mm'))\n",
    "\n",
    "# convert OFFMKTDATE to date\n",
    "df = df.withColumn('OFFMKTDATE', to_date('OFFMKTDATE', 'MM/dd/yyyy HH:mm'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=brown> Part 2: Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.0) Categorize columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PERCENT_OLDER_AGE_HOMES', 'ACRES', 'TAXWITHASSESSMENTS', 'BATHSTHREEQUARTER', 'PERCENT_SIMILAR_AGE_HOMES', 'GarageDescription', 'PERCENT_BIGGER_SIZE_HOMES', 'BATHQUARTER', 'YEARBUILT', 'LISTPRICE', 'LIVINGAREA', 'FOUNDATIONSIZE', 'SQFTABOVEGROUND', 'SQFTBELOWGROUND', 'BATHSTOTAL', 'OriginalListPrice', 'TAXES', 'ASSESSEDVALUATION', 'ASSOCIATIONFEE', 'FIREPLACES', 'PERCENT_SIMILAR_SIZE_HOMES', 'BEDROOMS', 'BATHSHALF', 'PERCENT_NEWER_AGE_HOMES', 'PERCENT_SMALLER_SIZE_HOMES', 'BATHSFULL', 'TAXYEAR']\n"
     ]
    }
   ],
   "source": [
    "# Columns to drop\n",
    "cols_to_drop = ['UNITNUMBER','Class','STREETNUMBERNUMERIC','LOTSIZEDIMENSIONS','MLSID','streetaddress','STREETNAME','PostalCode','StateOrProvince','PricePerTSFT','MapLetter','RoomFloor1','RoomFloor2','RoomFloor3','RoomFloor4','RoomFloor5','RoomFloor6','RoomFloor7','RoomFloor8']\n",
    "\n",
    "# Columns to extract features from\n",
    "cols_to_extract = ['FENCE','ROOF','PoolDescription','GARAGEDESCRIPTION','APPLIANCES','EXTERIOR','DiningRoomDescription','BASEMENT','BATHDESC','ZONING','CoolingDescription','ROOMFAMILYCHAR','roomtype']\n",
    "\n",
    "# Categorical columns for one hot encoding\n",
    "categorical_cols = ['CITY','LISTTYPE','SCHOOLDISTRICTNUMBER','PotentialShortSale','STYLE','AssumableMortgage','ASSESSMENTPENDING']\n",
    "\n",
    "# Temperary columns (not yet suitable as feature)\n",
    "temperary_cols = ['longitude','latitude','NO','PDOM','RoomArea1','RoomArea2','RoomArea3','RoomArea4','RoomArea5','RoomArea6','RoomArea7','RoomArea8','LISTDATE','OFFMKTDATE','DAYSONMARKET','BACKONMARKETDATE']\n",
    "\n",
    "# Label column (variable to be predicted)\n",
    "label_col = ['SALESCLOSEPRICE']\n",
    "\n",
    "# Feature columns (more to be added)\n",
    "feature_cols = list(set(df.columns)-set(cols_to_drop)-set(cols_to_extract)-set(categorical_cols)-set(temperary_cols)-set(label_col))\n",
    "print(feature_cols)  # 27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Drop unuseful columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop a list of unuseful columns (CLASS is constant)\n",
    "df = df.drop(*cols_to_drop)  # 76 -> 57"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) Use text filter to remove rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|  AssumableMortgage|\n",
      "+-------------------+\n",
      "|  Yes w/ Qualifying|\n",
      "| Information Coming|\n",
      "|               null|\n",
      "|Yes w/No Qualifying|\n",
      "|      Not Assumable|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (2.2.1) Use text filter to remove 'Not Disclosed' POTENTIALSHORTSALE\n",
    "df = df.where(~df['PotentialShortSale'].like('Not Disclosed'))\n",
    "#print(df.count())  # 5000 -> 4989\n",
    "\n",
    "# (2.2.2) Use text filters to remove ASSUMABLEMORTGAGE, which is an unusual occurrence in the real estate\n",
    "df.select(['AssumableMortgage']).distinct().show()\n",
    "\n",
    "# List of possible values containing 'yes'\n",
    "yes_values = ['Yes w/ Qualifying', 'Yes w/No Qualifying']\n",
    "\n",
    "# Filter the text values out of df but keep null values\n",
    "text_filter = ~df['AssumableMortgage'].isin(yes_values) | df['ASSUMABLEMORTGAGE'].isNull()\n",
    "df = df.where(text_filter)\n",
    "\n",
    "#print(df.count())  # 4989 -> 4965"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3) Find and remove outliers using skewness of normal distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(skewness(LISTPRICE)=2.788039060845876)]\n",
      "[Row(skewness(SALESCLOSEPRICE)=2.6213496232050693)]\n"
     ]
    }
   ],
   "source": [
    "# (2.3.1) Check the skewness (distortion from the normal distribution) of LISTPRICE and SALESCLOSEPRICE\n",
    "\"\"\"For Random Forest Regression, no need to convert variable to be standard normally distributed.\"\"\"\n",
    "from pyspark.sql.functions import skewness\n",
    "\n",
    "# Compute and print skewness of LISTPRICE\n",
    "print(df.agg({'LISTPRICE': 'skewness'}).collect())  # 1.20, positive-->left skewed, log(var)->correct left skew data to be normal\n",
    "\n",
    "# Compute and print skewness of SALESCLOSEPRICE\n",
    "print(df.agg({'SALESCLOSEPRICE': 'skewness'}).collect())  # 1.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2.3.2)  Filter out any outlier homes whose log_ScaledListPrice and log_ScaledClosePrice are significantly more or les than the average\n",
    "from pyspark.sql.functions import mean, stddev, log\n",
    "\n",
    "def remove_outlier(df, cols):\n",
    "    for col in cols:\n",
    "        # calculate values used for filtering\n",
    "        std_val = df.agg({col: 'stddev'}).collect()[0][0]\n",
    "        mean_val = df.agg({col: 'mean'}).collect()[0][0]\n",
    "        # create three std upper and lower bounds for data\n",
    "        hi_bound = mean_val + (3*std_val)\n",
    "        low_bound = mean_val - (3*std_val)\n",
    "        # use where() to filter the DataFrame between values\n",
    "        df = df.where((df[col]<hi_bound) & (df[col]>low_bound))\n",
    "    return df\n",
    "\n",
    "# Define logarithmic scaled list and close prices\n",
    "df = df.withColumn('log_ScaledListPrice',log(df['LISTPRICE']))\n",
    "df = df.withColumn('log_ScaledClosePrice', log(df['SALESCLOSEPRICE']))\n",
    "\n",
    "# Drop outlier LISTPRICE and log_ScaledClosePrice (both are left skewed, logarithmic scale turns them to be normally distributed)\n",
    "df = remove_outlier(df, ['log_ScaledListPrice', 'log_ScaledClosePrice'])\n",
    "\n",
    "# Print count of remaining records\n",
    "#print(df.count())  # 4965 -> 4924\n",
    "\n",
    "# Drop the log_ScaledListPrice and log_ScaledClosePrice, as we only use them to find outliers\n",
    "df = df.drop(*['log_ScaledListPrice','log_ScaledClosePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.4) Get more data from other DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2.4.1) Load location DataFrame that contains WALKSCORE and BIKESCORE etc.\n",
    "walk_df = spark.read.csv('../../data/location.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Cast data types\n",
    "walk_df = walk_df.withColumn('longitude', walk_df['longitude'].cast('double'))\n",
    "walk_df = walk_df.withColumn('latitude', walk_df['latitude'].cast('double'))\n",
    "\n",
    "# Round precision\n",
    "from pyspark.sql.functions import round\n",
    "df = df.withColumn('longitude', round(df['longitude'], 5))\n",
    "df = df.withColumn('latitude', round(df['latitude'], 5))\n",
    "\n",
    "# Create join condition\n",
    "condition = ['longitude','latitude']\n",
    "\n",
    "# Join the dataframes together\n",
    "df = df.join(walk_df, on=condition, how='left')\n",
    "\n",
    "# Add new columns to feature columns\n",
    "feature_cols += ['walkscore','bikescore']  # 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2.4.2) Add median home value from previous years at different cities\n",
    "from pyspark.sql.functions import year\n",
    "\n",
    "# Load price dataframe\n",
    "price_df = spark.read.csv('../../data/price.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Create year column\n",
    "df = df.withColumn('LIST_YEAR', year('LISTDATE'))\n",
    "\n",
    "# Adjust year to match\n",
    "df = df.withColumn('REPORT_YEAR', (df['LIST_YEAR'] - 1))\n",
    "\n",
    "# Create join condition\n",
    "condition = [df['CITY'] == price_df['City'], df['REPORT_YEAR'] == price_df['year']]\n",
    "\n",
    "# Join the dataframes together\n",
    "df = df.join(price_df, on=condition, how='left').drop(price_df.City)\n",
    "#df = df.drop('City')\n",
    "\n",
    "# Inspect that new columns are available\n",
    "#df[['MedianHomeValue']].show()\n",
    "\n",
    "# Add column to feature columns?\n",
    "feature_cols += ['MedianHomeValue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2.4.3) Add mortgage rates: same week as the list date, 1-week, 2-week, 3-week and 4-week ahead of list date\n",
    "\"\"\"PART I: define lagged mortgage rates\"\"\"  \n",
    "from pyspark.sql.functions import lag, datediff\n",
    "from pyspark.sql.window import Window\n",
    "  \n",
    "# Load mortgage dataframe and cast data type\n",
    "mort_df = spark.read.csv('../../data/mortgage.csv', header=True, inferSchema=True)\n",
    "mort_df = mort_df.withColumn('DATE', to_date(mort_df['DATE']))\n",
    "\n",
    "# Create window\n",
    "w = Window().orderBy(mort_df['DATE'])\n",
    "\n",
    "# Create lag columns of DATE and MORTGAGEUS30\n",
    "for i in range(4):\n",
    "    mort_df = mort_df.withColumn('MORTGAGE30US-'+str(i+1)+'WK', lag('MORTGAGE30US', count=i+1).over(w))\n",
    "\n",
    "# Calculate difference between date columns\n",
    "#mort_df = mort_df.withColumn('Days_Between_Report', datediff('DATE', 'DATE-1'))\n",
    "#mort_df.select('Days_Between_Report').distinct().show()\n",
    "\n",
    "\"\"\"PART II: define year and week of year for df and mort_df\"\"\"\n",
    "# Create week of year for df and mort_df\n",
    "from pyspark.sql.functions import to_date, weekofyear\n",
    "\n",
    "# Convert to date type\n",
    "mort_df = mort_df.withColumn('DATE', to_date('DATE', 'MM/dd/yyyy HH:mm'))\n",
    "\n",
    "# Get the year and week of the year\n",
    "mort_df = mort_df.withColumn('LIST_YEAR', year('DATE'))\n",
    "mort_df = mort_df.withColumn('LIST_WEEKOFYEAR', weekofyear('DATE'))\n",
    "\n",
    "# Drop column DATE\n",
    "mort_df = mort_df.drop('DATE')\n",
    "\n",
    "# Get the week of the year\n",
    "df = df.withColumn('LIST_WEEKOFYEAR', weekofyear('LISTDATE'))\n",
    "\n",
    "\"\"\"PART III: join df and mort_df on year and week of year\"\"\"\n",
    "# Join df and mort_df\n",
    "condition = ['LIST_YEAR', 'LIST_WEEKOFYEAR']\n",
    "df = df.join(mort_df, on=condition, how='left')\n",
    "\n",
    "# Add new columns to feature columns\n",
    "feature_cols += ['LIST_YEAR','MORTGAGE30US','MORTGAGE30US-1WK','MORTGAGE30US-2WK','MORTGAGE30US-3WK','MORTGAGE30US-4WK']  # 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.5) Generate features by combining fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2.5.1) Generate features by getting ratios between fields\n",
    "# ASSESSED_TO_LIST\n",
    "df = df.withColumn('ASSESSED_TO_LIST', (df['ASSESSEDVALUATION'] / df['LISTPRICE']))\n",
    "\n",
    "# TAX_TO_LIST\n",
    "df = df.withColumn('TAX_TO_LIST', (df['TAXES'] / df['LISTPRICE']))\n",
    "\n",
    "# BED_TO_BATHS\n",
    "df = df.withColumn('BED_TO_BATHS', (df['BEDROOMS'] / df['BATHSTOTAL']))\n",
    "\n",
    "# (2.5.2) Generate feature by adding two fields\n",
    "df = df.withColumn('SQFT_TOTAL', df['SQFTBELOWGROUND'] + df['SQFTABOVEGROUND'])\n",
    "\n",
    "# Add new columns to feature columns\n",
    "feature_cols += ['ASSESSED_TO_LIST','TAX_TO_LIST','BED_TO_BATHS','SQFT_TOTAL']  # 39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.5) Convert string features to numeric ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dimension description of rooms to numeric values\n",
    "from pyspark.sql.functions import regexp_replace, split\n",
    "\n",
    "# columns to be transformed\n",
    "dimension_cols = ['RoomArea1', 'RoomArea2','RoomArea3','RoomArea4','RoomArea5','RoomArea6','RoomArea7','RoomArea8']\n",
    "for col in dimension_cols:\n",
    "    # replace 'X' with 'x', split on 'x', then multiply the 2 values\n",
    "    df = df.withColumn(col, regexp_replace(col, 'X', 'x'))\n",
    "    df = df.withColumn(col, split(df[col], 'x'))\n",
    "    df = df.withColumn(col, df[col][0] * df[col][1])\n",
    "    \n",
    "# Add new columns to to feature columns\n",
    "feature_cols += dimension_cols  # 47"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.5) Extract features from free form text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PERCENT_OLDER_AGE_HOMES', 'ACRES', 'TAXWITHASSESSMENTS', 'BATHSTHREEQUARTER', 'PERCENT_SIMILAR_AGE_HOMES', 'GarageDescription', 'PERCENT_BIGGER_SIZE_HOMES', 'BATHQUARTER', 'YEARBUILT', 'LISTPRICE', 'LIVINGAREA', 'FOUNDATIONSIZE', 'SQFTABOVEGROUND', 'SQFTBELOWGROUND', 'BATHSTOTAL', 'OriginalListPrice', 'TAXES', 'ASSESSEDVALUATION', 'ASSOCIATIONFEE', 'FIREPLACES', 'PERCENT_SIMILAR_SIZE_HOMES', 'BEDROOMS', 'BATHSHALF', 'PERCENT_NEWER_AGE_HOMES', 'PERCENT_SMALLER_SIZE_HOMES', 'BATHSFULL', 'TAXYEAR', 'walkscore', 'bikescore', 'MedianHomeValue', 'LIST_YEAR', 'MORTGAGE30US', 'MORTGAGE30US-1WK', 'MORTGAGE30US-2WK', 'MORTGAGE30US-3WK', 'MORTGAGE30US-4WK', 'ASSESSED_TO_LIST', 'TAX_TO_LIST', 'BED_TO_BATHS', 'SQFT_TOTAL', 'RoomArea1', 'RoomArea2', 'RoomArea3', 'RoomArea4', 'RoomArea5', 'RoomArea6', 'RoomArea7', 'RoomArea8', 'FENCE_Partial', 'FENCE_Rail', 'FENCE_Wire', 'FENCE_Privacy', 'FENCE_Chain Link', 'FENCE_Invisible', 'FENCE_Wood', 'FENCE_Full', 'FENCE_Electric', 'FENCE_Other', 'FENCE_None', 'ROOF_Pitched', 'ROOF_Shakes', 'ROOF_Tar/Gravel', 'ROOF_Rubber', 'ROOF_Age Over 8 Years', 'ROOF_Metal', 'ROOF_Tile', 'ROOF_Asphalt Shingles', 'ROOF_Wood Shingles', 'ROOF_Unspecified Shingle', 'ROOF_Age 8 Years or Less', 'ROOF_Other', 'ROOF_Flat', 'ROOF_Slate']\n"
     ]
    }
   ],
   "source": [
    "# Split and explode, and pivot and join\n",
    "from pyspark.sql.functions import explode, lit, coalesce, first\n",
    "\n",
    "def extract_features(df, col):\n",
    "    \"\"\"\n",
    "    Extract features from free_form text\n",
    "    \"\"\"\n",
    "    global extracted_features  # can be changed within functions\n",
    "    \n",
    "    # convert string to list-like array\n",
    "    df = df.withColumn(col+'_list', split(df[col], ', '))\n",
    "\n",
    "    # explode the values into new records\n",
    "    ex_df = df.withColumn('ex_'+col+'_list', explode(df[col+'_list']))\n",
    "\n",
    "    # create a dummy column of constant value\n",
    "    ex_df = ex_df.withColumn('constant_val', lit(1))\n",
    "\n",
    "    # pivot the values \n",
    "    piv_df = ex_df.groupBy('NO').pivot('ex_'+col+'_list').agg(coalesce(first('constant_val')))\n",
    "\n",
    "    # rename pivoted column names except 'NO' for join operations later\n",
    "    for col_name in list(set(piv_df.columns)-set(['NO'])):\n",
    "        piv_df = piv_df.withColumnRenamed(col_name, col+'_'+col_name)\n",
    "    \n",
    "    # add new columns to feature columns\n",
    "    extracted_features += list(set(piv_df.columns)-set(['NO']))\n",
    "    \n",
    "    # drop col_list\n",
    "    df = df.drop(col+'_list')\n",
    "    \n",
    "    # join the dataframes and fill null\n",
    "    df = df.join(piv_df, on='NO', how='left')\n",
    "\n",
    "    # columns to zero fill\n",
    "    zfill_cols = piv_df.columns\n",
    "\n",
    "    # zero fill the pivoted values\n",
    "    df = df.fillna(0, subset=zfill_cols)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Define a list of extracted feature names\n",
    "extracted_features = []\n",
    "\n",
    "#======================================================#\n",
    "# DUE TO LIMITED MEMEORY, ONLY EXTRACT FIRST 2 COLUMNS #\n",
    "#======================================================#\n",
    "# Extract features from selected columns\n",
    "for col in cols_to_extract[0:2]:\n",
    "    df = extract_features(df, col)\n",
    "    \n",
    "# Add new columns to feature columns\n",
    "feature_cols += extracted_features\n",
    "    \n",
    "print(feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.6) Binarizing to distinguish if house is listed on a week day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed functions\n",
    "from pyspark.sql.functions import to_timestamp, dayofweek\n",
    "\n",
    "# Convert to date type (already converted)\n",
    "#df = df.withColumn('LISTDATE', to_date('LISTDATE', 'MM/dd/yyyy HH:mm'))\n",
    "\n",
    "# Get the day of the week\n",
    "df = df.withColumn('LIST_DAYOFWEEK', dayofweek('LISTDATE'))\n",
    "\n",
    "# Import transformer\n",
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "# Create the transformer (>5 --> 1)\n",
    "binarizer = Binarizer(threshold=5.0, inputCol='LIST_DAYOFWEEK', outputCol='Listed_On_Weekend')\n",
    "\n",
    "# Apply the transformation to df\n",
    "df = df.withColumn('LIST_DAYOFWEEK', df['LIST_DAYOFWEEK'].cast('double'))\n",
    "df = binarizer.transform(df)  # 'LIST_DAYOFWEEK' must be double\n",
    "\n",
    "# Add new columns to feature columns\n",
    "feature_cols += ['LIST_DAYOFWEEK']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.7) Bucketing of BEDROOMS to 7 'bukets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "# Create the bucket splits and bucketizer\n",
    "splits = [0, 1, 2, 3, 4, 5, float('Inf')]\n",
    "buck = Bucketizer(splits=splits, inputCol='BEDROOMS', outputCol='bedrooms')\n",
    "\n",
    "# Apply the transformation to df\n",
    "df = buck.transform(df)\n",
    "\n",
    "# Rename bedrooms\n",
    "df = df.drop('BEDROOMS')\n",
    "df = df.withColumnRenamed('bedrooms','BEDROOMS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.8) One hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NO', 'LIST_YEAR', 'LIST_WEEKOFYEAR', 'longitude', 'latitude', 'SALESCLOSEPRICE', 'LISTDATE', 'LISTPRICE', 'OriginalListPrice', 'FOUNDATIONSIZE', 'FENCE', 'DAYSONMARKET', 'OFFMKTDATE', 'FIREPLACES', 'RoomArea4', 'roomtype', 'ROOF', 'PoolDescription', 'PDOM', 'GarageDescription', 'SQFTABOVEGROUND', 'TAXES', 'RoomArea1', 'TAXWITHASSESSMENTS', 'TAXYEAR', 'LIVINGAREA', 'YEARBUILT', 'ZONING', 'ACRES', 'CoolingDescription', 'APPLIANCES', 'BACKONMARKETDATE', 'ROOMFAMILYCHAR', 'RoomArea3', 'EXTERIOR', 'RoomArea2', 'DiningRoomDescription', 'BASEMENT', 'BATHSFULL', 'BATHSHALF', 'BATHQUARTER', 'BATHSTHREEQUARTER', 'BATHSTOTAL', 'BATHDESC', 'RoomArea5', 'RoomArea6', 'RoomArea7', 'RoomArea8', 'SQFTBELOWGROUND', 'ASSOCIATIONFEE', 'ASSESSEDVALUATION', 'PERCENT_OLDER_AGE_HOMES', 'PERCENT_SIMILAR_AGE_HOMES', 'PERCENT_NEWER_AGE_HOMES', 'PERCENT_BIGGER_SIZE_HOMES', 'PERCENT_SIMILAR_SIZE_HOMES', 'PERCENT_SMALLER_SIZE_HOMES', 'walkscore', 'bikescore', 'transitscore', 'REPORT_YEAR', 'MedianHomeValue', 'Year', 'MORTGAGE30US', 'MORTGAGE30US-1WK', 'MORTGAGE30US-2WK', 'MORTGAGE30US-3WK', 'MORTGAGE30US-4WK', 'ASSESSED_TO_LIST', 'TAX_TO_LIST', 'BED_TO_BATHS', 'SQFT_TOTAL', 'FENCE_Chain Link', 'FENCE_Electric', 'FENCE_Full', 'FENCE_Invisible', 'FENCE_None', 'FENCE_Other', 'FENCE_Partial', 'FENCE_Privacy', 'FENCE_Rail', 'FENCE_Wire', 'FENCE_Wood', 'ROOF_Age 8 Years or Less', 'ROOF_Age Over 8 Years', 'ROOF_Asphalt Shingles', 'ROOF_Flat', 'ROOF_Metal', 'ROOF_Other', 'ROOF_Pitched', 'ROOF_Rubber', 'ROOF_Shakes', 'ROOF_Slate', 'ROOF_Tar/Gravel', 'ROOF_Tile', 'ROOF_Unspecified Shingle', 'ROOF_Wood Shingles', 'LIST_DAYOFWEEK', 'Listed_On_Weekend', 'CITY_IDX', 'LISTTYPE_IDX', 'SCHOOLDISTRICTNUMBER_IDX', 'PotentialShortSale_IDX', 'STYLE_IDX', 'AssumableMortgage_IDX', 'ASSESSMENTPENDING_IDX', 'CITY_Vec', 'LISTTYPE_Vec', 'SCHOOLDISTRICTNUMBER_Vec', 'PotentialShortSale_Vec', 'STYLE_Vec', 'AssumableMortgage_Vec', 'ASSESSMENTPENDING_Vec']\n",
      "+--------+-------------+\n",
      "|CITY_IDX|     CITY_Vec|\n",
      "+--------+-------------+\n",
      "|     4.0|(5,[4],[1.0])|\n",
      "|     2.0|(5,[2],[1.0])|\n",
      "|     2.0|(5,[2],[1.0])|\n",
      "|     2.0|(5,[2],[1.0])|\n",
      "|     3.0|(5,[3],[1.0])|\n",
      "|     0.0|(5,[0],[1.0])|\n",
      "|     0.0|(5,[0],[1.0])|\n",
      "|     0.0|(5,[0],[1.0])|\n",
      "|     0.0|(5,[0],[1.0])|\n",
      "|     0.0|(5,[0],[1.0])|\n",
      "|     0.0|(5,[0],[1.0])|\n",
      "|     0.0|(5,[0],[1.0])|\n",
      "|     0.0|(5,[0],[1.0])|\n",
      "|     0.0|(5,[0],[1.0])|\n",
      "|     0.0|(5,[0],[1.0])|\n",
      "|     0.0|(5,[0],[1.0])|\n",
      "|     0.0|(5,[0],[1.0])|\n",
      "|     0.0|(5,[0],[1.0])|\n",
      "|     0.0|(5,[0],[1.0])|\n",
      "|     0.0|(5,[0],[1.0])|\n",
      "+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For categorical values, we can just map the text values to numbers, the use one hot encoding to transform them to numeric vectors\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "# (a) Create list of StringIndexers using list comprehension\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+'_IDX').setHandleInvalid(\"keep\") for col in categorical_cols]\n",
    "\n",
    "# Create pipeline of indexers\n",
    "indexer_pipeline = Pipeline(stages=indexers)\n",
    "\n",
    "# Fit and Transform the pipeline to the original data\n",
    "df = indexer_pipeline.fit(df).transform(df)\n",
    "\n",
    "# Clean up redundant columns no longer needed\n",
    "df = df.drop(*categorical_cols)\n",
    "\n",
    "# Inspect data transformations\n",
    "#print(df.dtypes)\n",
    "\n",
    "# (b) One hot encode indexed values\n",
    "encoders = [OneHotEncoder(inputCol=col+'_IDX', outputCol=col+'_Vec') for col in categorical_cols]\n",
    "\n",
    "# Create pipeline of indexers\n",
    "encoder_pipeline = Pipeline(stages=encoders)\n",
    "\n",
    "# Fit and Transform the pipeline to the data\n",
    "df = encoder_pipeline.fit(df).transform(df)\n",
    "\n",
    "# Add new columns to feature columns\n",
    "feature_cols += [col+'_Vec' for col in categorical_cols]\n",
    "\n",
    "print(df.columns)\n",
    "df.select(['CITY_IDX','CITY_Vec']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.4) Drop columns with low observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2.4.1) Automate dropping columns if they are missing data beyond a specific threshold\n",
    "def column_dropper(df, threshold):\n",
    "    # Takes a dataframe and threshold for missing values. Returns a dataframe.\n",
    "    total_records = df.count()\n",
    "    for col in df.columns:\n",
    "        # Calculate the percentage of missing values\n",
    "        missing = df.where(df[col].isNull()).count()\n",
    "        missing_percent = missing / total_records\n",
    "        # Drop column if percent of missing is more than threshold\n",
    "        if missing_percent > threshold:\n",
    "            df = df.drop(col)\n",
    "    return df\n",
    "\n",
    "# Drop columns that are more than 60% missing\n",
    "df = column_dropper(df, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2.4.2) Drop columns with observations less than 30 (minimum number for statistical significance)\n",
    "# Get the names of binary columns from extracted features\n",
    "binary_cols = extracted_features\n",
    "\n",
    "# Get column\n",
    "obs_threshold = 30\n",
    "cols_to_remove = list()\n",
    "for col in binary_cols:\n",
    "    # Count the number of 1 values in the binary column\n",
    "    obs_count = df.agg({col: 'sum'}).collect()[0][0]\n",
    "    # If less than our observation threshold, remove\n",
    "    if obs_count < obs_threshold:\n",
    "        cols_to_remove.append(col)\n",
    "    \n",
    "# Drop columns\n",
    "df = df.drop(*cols_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.5) Naively handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values (missing values in categoricals are filled with 0 already)\n",
    "#df = df.fillna(-1, subset=df.columns)\n",
    "df = df.fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=brown> Part 3: Split data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|SALESCLOSEPRICE|            features|\n",
      "+---------------+--------------------+\n",
      "|         799000|[5586.0,7570.0,55...|\n",
      "|         244000|[1651.0,2682.0,16...|\n",
      "|         255000|[1776.0,3424.0,17...|\n",
      "|         250000|[1567.0,2672.0,15...|\n",
      "|         231700|[1750.0,2283.0,17...|\n",
      "|          81400|[724.0,1012.0,724...|\n",
      "|         120000|[920.0,1358.0,920...|\n",
      "|         135000|[1096.0,1612.0,10...|\n",
      "|         145000|[884.0,1635.0,884...|\n",
      "|         150730|[1550.0,1431.0,15...|\n",
      "|         160000|[900.0,2127.0,900...|\n",
      "|         160000|[864.0,1891.0,864...|\n",
      "|         180000|[2171.0,1155.0,21...|\n",
      "|         200000|[944.0,2264.0,944...|\n",
      "|         206925|[1620.0,1482.0,16...|\n",
      "|         190000|[1146.0,2388.0,11...|\n",
      "|         214000|[1972.0,2170.0,19...|\n",
      "|         245000|[1452.0,2190.0,14...|\n",
      "|         279000|[1825.0,3354.0,18...|\n",
      "|         547750|[2916.0,8178.0,29...|\n",
      "+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (3.0) PySpark ML algorithms require all of the features to be provided in a single column of type vector.\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Define the columsn to be converted to vectors (slimmed down features)\n",
    "feature_cols = ['SQFT_TOTAL','TAXES','LIVINGAREA','SQFTABOVEGROUND','BATHSTOTAL','YEARBUILT','FIREPLACES','BATHSHALF','walkscore']  \n",
    "\n",
    "# Create teh vector assembler transformer\n",
    "vec = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "\n",
    "# Apply the vector transformer to data ('features' column will be added)\n",
    "df = vec.transform(df)\n",
    "\n",
    "# Select only the feature vectors and the dependent variable\n",
    "ml_ready_df = df.select(['SALESCLOSEPRICE','features'])\n",
    "ml_ready_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3.1) Train on all available data except for the last 45 days, which you want to use for a test set.\n",
    "from datetime import timedelta\n",
    "from pyspark.sql.functions import datediff, to_date, lit\n",
    "\n",
    "def train_test_split_date(df, split_col, test_days=45):\n",
    "    \"\"\"Calculate the date to split test and training sets\"\"\"\n",
    "    # Find how many days our data spans\n",
    "    max_date = df.agg({split_col: 'max'}).collect()[0][0]\n",
    "    min_date = df.agg({split_col: 'min'}).collect()[0][0]\n",
    "    # Subtract an integer number of days from the last date in dataset\n",
    "    split_date = max_date - timedelta(days=test_days)\n",
    "    return split_date\n",
    "\n",
    "# Find the date to use in spitting test and train\n",
    "split_date = train_test_split_date(df, 'OFFMKTDATE')\n",
    "\n",
    "# Create Sequential Test and Training Sets\n",
    "train_df = df.where(df['OFFMKTDATE'] < split_date)\n",
    "test_df = df.where(df['OFFMKTDATE'] >= split_date).where(df['LISTDATE'] <= split_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------+\n",
      "|  LISTDATE|OFFMKTDATE|DAYSONMARKET|\n",
      "+----------+----------+------------+\n",
      "|2017-12-07|2017-12-18|           3|\n",
      "|2017-10-24|2017-12-19|          47|\n",
      "|2017-10-05|2017-12-19|          66|\n",
      "|2017-11-17|2017-12-13|          23|\n",
      "|2017-07-27|2017-12-13|         136|\n",
      "|2017-08-02|2017-12-13|         130|\n",
      "|2017-11-13|2017-12-18|          27|\n",
      "|2017-11-28|2018-01-02|          12|\n",
      "|2017-12-10|2017-12-14|           0|\n",
      "|2017-09-25|2017-12-14|          76|\n",
      "|2017-07-27|2017-12-20|         136|\n",
      "|2017-11-29|2017-12-18|          11|\n",
      "|2017-08-14|2018-01-09|         118|\n",
      "|2017-11-22|2018-01-08|          18|\n",
      "|2017-11-17|2017-12-22|          23|\n",
      "|2017-11-27|2017-12-27|          13|\n",
      "|2017-10-23|2017-12-12|          48|\n",
      "|2017-07-05|2017-12-19|         158|\n",
      "|2017-10-06|2017-12-14|          65|\n",
      "|2017-12-04|2017-12-17|           6|\n",
      "+----------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (3.2) Adjust time feature DAYSONMARKET\n",
    "# Recalculate DAYSONMARKET from what we know on our split date\n",
    "test_df = test_df.withColumn('DAYSONMARKET', datediff(lit(split_date), 'LISTDATE'))\n",
    "\n",
    "# Review the difference\n",
    "test_df[['LISTDATE', 'OFFMKTDATE', 'DAYSONMARKET']].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=brown> Part 4: Train model, make predictions, and evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4.1.1) Train a Random Forest Regressor (RFR) and a GBT Regressor model\n",
    "from pyspark.ml.regression import RandomForestRegressor, GBTRegressor\n",
    "\n",
    "# initialize a RFR model with columns to utilize\n",
    "rfr = RandomForestRegressor(featuresCol=\"features\",\n",
    "                            labelCol=\"SALESCLOSEPRICE\",\n",
    "                            predictionCol=\"Prediction_Price\",\n",
    "                            seed=42)\n",
    "\n",
    "# train the RFR model\n",
    "rfr_model = rfr.fit(train_df)\n",
    "\n",
    "# make predictions\n",
    "rfr_predictions = rfr_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4.1.2) Train a Gradient Boosted Trees (GBT) model\n",
    "gbt = GBTRegressor(featuresCol='features',\n",
    "                   labelCol='SALESCLOSEPRICE',\n",
    "                   predictionCol=\"Prediction_Price\",\n",
    "                   seed=42)\n",
    "\n",
    "# train the GBT model\n",
    "gbt_model = gbt.fit(train_df)\n",
    "\n",
    "# make predictions\n",
    "gbt_predictions = gbt_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+\n",
      "|  Prediction_Price|SALESCLOSEPRICE|\n",
      "+------------------+---------------+\n",
      "| 477932.5759685775|         415000|\n",
      "|151340.02363332474|          85000|\n",
      "| 587096.6799408385|         625000|\n",
      "|188066.02407784035|         170000|\n",
      "|213134.80352795072|         140000|\n",
      "|178352.33411807456|         162500|\n",
      "|187511.62608683787|         205000|\n",
      "|249954.92224734588|         275000|\n",
      "|157001.55870608546|         109900|\n",
      "|181616.74266121833|         166140|\n",
      "| 223712.8217376111|         214900|\n",
      "|164231.34927270835|         146000|\n",
      "|145066.02885670474|          78900|\n",
      "|181746.24445993835|         240000|\n",
      "|382794.44830720796|         385000|\n",
      "| 485965.2284785349|         625000|\n",
      "| 200371.6124723446|         230000|\n",
      "| 298326.4769805313|         259000|\n",
      "|415643.60868714296|         398000|\n",
      "|149100.89447796842|         133000|\n",
      "+------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------------+---------------+\n",
      "|  Prediction_Price|SALESCLOSEPRICE|\n",
      "+------------------+---------------+\n",
      "| 451062.5082536567|         415000|\n",
      "|126846.34527473673|          85000|\n",
      "| 510253.4612519609|         625000|\n",
      "|173141.37803999128|         170000|\n",
      "|176790.75632046917|         140000|\n",
      "|168646.97487481864|         162500|\n",
      "|172774.57557357897|         205000|\n",
      "|264922.15870568476|         275000|\n",
      "|167463.91875759748|         109900|\n",
      "|171630.15163004314|         166140|\n",
      "|219800.53111083448|         214900|\n",
      "| 141694.9649637782|         146000|\n",
      "|113872.34409156631|          78900|\n",
      "|175168.33926727634|         240000|\n",
      "| 344871.5419213124|         385000|\n",
      "| 507784.9636767676|         625000|\n",
      "| 223377.1583111434|         230000|\n",
      "| 301788.5726198996|         259000|\n",
      "|474028.81539062253|         398000|\n",
      "|153960.34177801845|         133000|\n",
      "+------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (4.2) Inspect results\n",
    "rfr_predictions.select('Prediction_Price','SALESCLOSEPRICE').show()\n",
    "gbt_predictions.select('Prediction_Price','SALESCLOSEPRICE').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosted Trees RMSE: 73592.23159304303\n",
      "Gradient Boosted Trees R^2: 0.6555645898757021\n",
      "Random Forest Regression RMSE: 60045.794225661586\n",
      "Random Forest Regression R^2: 0.770697370293886\n"
     ]
    }
   ],
   "source": [
    "# (4.3) Evaluate a model\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# select columns to compute test error\n",
    "evaluator = RegressionEvaluator(labelCol='SALESCLOSEPRICE',\n",
    "                                predictionCol='Prediction_Price')\n",
    "\n",
    "# dictionary of model predictions to loop over\n",
    "models = {'Gradient Boosted Trees': gbt_predictions, 'Random Forest Regression': rfr_predictions}\n",
    "\n",
    "for key, preds in models.items():\n",
    "    # create evaluation metrics\n",
    "    rmse = evaluator.evaluate(preds, {evaluator.metricName: 'rmse'})\n",
    "    r2 = evaluator.evaluate(preds, {evaluator.metricName: 'r2'})\n",
    "    # print Model Metrics\n",
    "    print(key + ' RMSE: ' + str(rmse))\n",
    "    print(key + ' R^2: ' + str(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=brown> Part 5: Interpreting, saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   importance          feature\n",
      "0    0.312659       SQFT_TOTAL\n",
      "1    0.274874            TAXES\n",
      "2    0.219070       LIVINGAREA\n",
      "3    0.070935  SQFTABOVEGROUND\n",
      "5    0.049717        YEARBUILT\n",
      "   importance          feature\n",
      "1    0.263247            TAXES\n",
      "0    0.193094       SQFT_TOTAL\n",
      "8    0.178850        walkscore\n",
      "5    0.166326        YEARBUILT\n",
      "3    0.087873  SQFTABOVEGROUND\n"
     ]
    }
   ],
   "source": [
    "# (5.1) Interpreting a model\n",
    "import pandas as pd\n",
    "\n",
    "for model in [rfr_model, gbt_model]:\n",
    "    # convert feature importances to a pandas column\n",
    "    fi_df = pd.DataFrame(model.featureImportances.toArray(),\n",
    "                         columns=['importance'])\n",
    "    \n",
    "    # convert list of features names to pandas column \n",
    "    fi_df['feature'] = pd.Series(feature_cols)\n",
    "    \n",
    "    # sort the data based on feature importance\n",
    "    fi_df.sort_values(by=['importance'], ascending=False, inplace=True)\n",
    "\n",
    "    # interpret results\n",
    "    print(fi_df.head())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5.2) Saving and loading models\n",
    "# save model\n",
    "rfr_model.save('rfr_real_estate_model')\n",
    "gbt_model.save('gbt_real_estate_model')\n",
    "\n",
    "# load model\n",
    "from pyspark.ml.regression import RandomForestRegressionModel, GBTRegressionModel\n",
    "\n",
    "rfr_model2 = RandomForestRegressionModel.load('rfr_real_estate_model')\n",
    "gbt_model2 = GBTRegressionModel.load('gbt_real_estate_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
